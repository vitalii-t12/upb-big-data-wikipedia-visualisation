version: "3"

services:
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    build: ./hadoop/namenode
    container_name: namenode
    restart: always
    ports:
      - 9870:9870
      - 9000:9000
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
      - ./spark-scripts:/spark-scripts
      - ./hadoop-config:/etc/hadoop
    environment:
      - HADOOP_USER_NAME=hdfs
      - CLUSTER_NAME=test
    env_file:
      - hadoop/hadoop.env
    networks:
      bigdata-network:
        ipv4_address: 192.168.1.10
    extra_hosts:
      - "namenode:192.168.1.10"
      - "datanode:192.168.1.11"
      - "resourcemanager:192.168.1.12"
      - "nodemanager:192.168.1.13"
      - "historyserver:192.168.1.14"
      - "spark-master:192.168.1.15"
      - "spark-worker:192.168.1.16"
      - "zeppelin:192.168.1.17"
      - "airflow-webserver:192.168.1.18"
      - "airflow-scheduler:192.168.1.19"
      - "airflow-postgres:192.168.1.20"


  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    build: ./hadoop/datanode
    container_name: datanode
    restart: always
    volumes:
      - hadoop_datanode:/hadoop/dfs/data
      - ./hadoop-config:/etc/hadoop
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
    env_file:
      - hadoop/hadoop.env
    networks:
      bigdata-network:
        ipv4_address: 192.168.1.11
    extra_hosts:
      - "namenode:192.168.1.10"
      - "datanode:192.168.1.11"
      - "resourcemanager:192.168.1.12"
      - "nodemanager:192.168.1.13"
      - "historyserver:192.168.1.14"
      - "spark-master:192.168.1.15"
      - "spark-worker:192.168.1.16"
      - "zeppelin:192.168.1.17"
      - "airflow-webserver:192.168.1.18"
      - "airflow-scheduler:192.168.1.19"
      - "airflow-postgres:192.168.1.20"

  resourcemanager:
    image: bde2020/hadoop-resourcemanager:2.0.0-hadoop3.2.1-java8
    build: ./hadoop/resourcemanager
    container_name: resourcemanager
    restart: always
    environment:
      SERVICE_PRECONDITION: "namenode:9000 namenode:9870 datanode:9864"
    volumes:
      - ./hadoop-config:/etc/hadoop
    env_file:
      - hadoop/hadoop.env
    networks:
      bigdata-network:
        ipv4_address: 192.168.1.12
    extra_hosts:
      - "namenode:192.168.1.10"
      - "datanode:192.168.1.11"
      - "resourcemanager:192.168.1.12"
      - "nodemanager:192.168.1.13"
      - "historyserver:192.168.1.14"
      - "spark-master:192.168.1.15"
      - "spark-worker:192.168.1.16"
      - "zeppelin:192.168.1.17"
      - "airflow-webserver:192.168.1.18"
      - "airflow-scheduler:192.168.1.19"
      - "airflow-postgres:192.168.1.20"

  nodemanager1:
    image: bde2020/hadoop-nodemanager:2.0.0-hadoop3.2.1-java8
    build: ./hadoop/nodemanager
    container_name: nodemanager
    restart: always
    environment:
      SERVICE_PRECONDITION: "namenode:9000 namenode:9870 datanode:9864 resourcemanager:8088"
    volumes:
      - ./hadoop-config:/etc/hadoop
    env_file:
      - hadoop/hadoop.env
    networks:
      bigdata-network:
        ipv4_address: 192.168.1.13
    extra_hosts:
      - "namenode:192.168.1.10"
      - "datanode:192.168.1.11"
      - "resourcemanager:192.168.1.12"
      - "nodemanager:192.168.1.13"
      - "historyserver:192.168.1.14"
      - "spark-master:192.168.1.15"
      - "spark-worker:192.168.1.16"
      - "zeppelin:192.168.1.17"
      - "airflow-webserver:192.168.1.18"
      - "airflow-scheduler:192.168.1.19"
      - "airflow-postgres:192.168.1.20"

  historyserver:
    image: bde2020/hadoop-historyserver:2.0.0-hadoop3.2.1-java8
    build: ./hadoop/historyserver
    container_name: historyserver
    restart: always
    environment:
      SERVICE_PRECONDITION: "namenode:9000 namenode:9870 datanode:9864 resourcemanager:8088"
    volumes:
      - hadoop_historyserver:/hadoop/yarn/timeline
      - ./hadoop-config:/etc/hadoop
    env_file:
      - hadoop/hadoop.env
    networks:
      bigdata-network:
        ipv4_address: 192.168.1.14
    extra_hosts:
      - "namenode:192.168.1.10"
      - "datanode:192.168.1.11"
      - "resourcemanager:192.168.1.12"
      - "nodemanager:192.168.1.13"
      - "historyserver:192.168.1.14"
      - "spark-master:192.168.1.15"
      - "spark-worker:192.168.1.16"
      - "zeppelin:192.168.1.17"
      - "airflow-webserver:192.168.1.18"
      - "airflow-scheduler:192.168.1.19"
      - "airflow-postgres:192.168.1.20"

  spark-master:
    container_name: spark-master
    build: spark/spark-master
    ports:
      - "8090:8080"  # Spark Master Web UI
      - "7077:7077"  # Spark Master Port
    environment:
      - INIT_DAEMON_STEP=setup_spark
      - HADOOP_CONF_DIR=/etc/hadoop  # Point to the mounted configuration directory
      - YARN_CONF_DIR=/etc/hadoop
    volumes:
      - ./spark-scripts:/spark-scripts
      - ./hadoop-config:/etc/hadoop
    networks:
      bigdata-network:
        ipv4_address: 192.168.1.15
    extra_hosts:
      - "namenode:192.168.1.10"
      - "datanode:192.168.1.11"
      - "resourcemanager:192.168.1.12"
      - "nodemanager:192.168.1.13"
      - "historyserver:192.168.1.14"
      - "spark-master:192.168.1.15"
      - "spark-worker:192.168.1.16"
      - "zeppelin:192.168.1.17"
      - "airflow-webserver:192.168.1.18"
      - "airflow-scheduler:192.168.1.19"
      - "airflow-postgres:192.168.1.20"

  spark-worker:
    container_name: spark-worker
    build: spark/spark-worker
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - HADOOP_CONF_DIR=/etc/hadoop  # Point to the mounted configuration directory
      - YARN_CONF_DIR=/etc/hadoop
    depends_on:
      - spark-master
    volumes:
      - ./spark-scripts:/spark-scripts
      - ./hadoop-config:/etc/hadoop
    networks:
      bigdata-network:
        ipv4_address: 192.168.1.16
    extra_hosts:
      - "namenode:192.168.1.10"
      - "datanode:192.168.1.11"
      - "resourcemanager:192.168.1.12"
      - "nodemanager:192.168.1.13"
      - "historyserver:192.168.1.14"
      - "spark-master:192.168.1.15"
      - "spark-worker:192.168.1.16"
      - "zeppelin:192.168.1.17"
      - "airflow-webserver:192.168.1.18"
      - "airflow-scheduler:192.168.1.19"
      - "airflow-postgres:192.168.1.20"

  zeppelin:
    build: ./zeppelin
    container_name: zeppelin
    ports:
      - "8080:8080" # Zeppelin UI
      - "4040-4050:4040-4050" # Spark UI
    environment:
      - SPARK_LOCAL_IP=0.0.0.0
      - SPARK_PUBLIC_DNS=zeppelin
      - HDFS_HOST=namenode:9870  # Pass the Namenode address
    volumes:
      - zeppelin_data:/zeppelin/data  # Mount data directory
      - ./zeppelin-scripts:/zeppelin/scripts  # Mount scripts directory
      - ./zeppelin/notebook:/opt/zeppelin/notebook  # Mount notebooks directory
      - ./hadoop-config:/etc/hadoop
    networks:
      bigdata-network:
        ipv4_address: 192.168.1.17
    extra_hosts:
      - "namenode:192.168.1.10"
      - "datanode:192.168.1.11"
      - "resourcemanager:192.168.1.12"
      - "nodemanager:192.168.1.13"
      - "historyserver:192.168.1.14"
      - "spark-master:192.168.1.15"
      - "spark-worker:192.168.1.16"
      - "zeppelin:192.168.1.17"
      - "airflow-webserver:192.168.1.18"
      - "airflow-scheduler:192.168.1.19"
      - "airflow-postgres:192.168.1.20"

  # Add Airflow service
  airflow-init:
    image: apache/airflow:2.7.0
    container_name: airflow-init
    entrypoint: airflow db init
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
    depends_on:
      - postgres
    networks:
      - bigdata-network

  airflow-webserver:
    image: apache/airflow:2.7.0
    container_name: airflow-webserver
    depends_on:
      - airflow-init
      - airflow-scheduler
      - postgres
    restart: always
    ports:
      - "8081:8080"  # Airflow UI
    volumes:
      - ./airflow/dags:/opt/airflow/dags  # Mount your DAGs directory
      - ./airflow/logs:/opt/airflow/logs  # Mount logs directory
      - ./airflow/plugins:/opt/airflow/plugins  # Mount plugins directory
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
#      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
      - AIRFLOW__CORE__FERNET_KEY=your_fernet_key_here
      - AIRFLOW__WEBSERVER__WORKER_TIMEOUT=300
      - AIRFLOW__CORE__LOGGING_LEVEL=INFO
    networks:
      bigdata-network:
        ipv4_address: 192.168.1.18
    extra_hosts:
      - "namenode:192.168.1.10"
      - "datanode:192.168.1.11"
      - "resourcemanager:192.168.1.12"
      - "nodemanager:192.168.1.13"
      - "historyserver:192.168.1.14"
      - "spark-master:192.168.1.15"
      - "spark-worker:192.168.1.16"
      - "zeppelin:192.168.1.17"
      - "airflow-webserver:192.168.1.18"
      - "airflow-scheduler:192.168.1.19"
      - "airflow-postgres:192.168.1.20"
    healthcheck:
        test: ["CMD-SHELL", "[ -f /opt/airflow/airflow-webserver.pid ]"]
        interval: "20s"
        timeout: "20s"
        retries: 3

  airflow-scheduler:
    image: apache/airflow:2.7.0
    container_name: airflow-scheduler
    restart: always
    depends_on:
      - airflow-init
      - postgres
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
#      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
      - AIRFLOW__CORE__FERNET_KEY=your_fernet_key_here
    networks:
      bigdata-network:
        ipv4_address: 192.168.1.19
    extra_hosts:
      - "namenode:192.168.1.10"
      - "datanode:192.168.1.11"
      - "resourcemanager:192.168.1.12"
      - "nodemanager:192.168.1.13"
      - "historyserver:192.168.1.14"
      - "spark-master:192.168.1.15"
      - "spark-worker:192.168.1.16"
      - "zeppelin:192.168.1.17"
      - "airflow-webserver:192.168.1.18"
      - "airflow-scheduler:192.168.1.19"
      - "airflow-postgres:192.168.1.20"

  postgres:
    image: postgres:14
    container_name: airflow-postgres
    restart: always
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      bigdata-network:
        ipv4_address: 192.168.1.20
    extra_hosts:
      - "namenode:192.168.1.10"
      - "datanode:192.168.1.11"
      - "resourcemanager:192.168.1.12"
      - "nodemanager:192.168.1.13"
      - "historyserver:192.168.1.14"
      - "spark-master:192.168.1.15"
      - "spark-worker:192.168.1.16"
      - "zeppelin:192.168.1.17"
      - "airflow-webserver:192.168.1.18"
      - "airflow-scheduler:192.168.1.19"
      - "airflow-postgres:192.168.1.20"


volumes:
  hadoop_namenode:
  hadoop_datanode:
  hadoop_historyserver:
  zeppelin_data:
  postgres_data:
#  zeppelin_notebooks:

networks:
  bigdata-network:
    driver: bridge
    ipam:
      config:
        - subnet: 192.168.1.0/24
